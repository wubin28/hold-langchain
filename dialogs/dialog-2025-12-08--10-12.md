## Me:

```markdown
很好。请照此阅读下面发表于2025年12月批评LangChain的图书中的相关章节,然后针对该章节中列举的LangChain把简单的事情搞复杂的各个场景(如FewShotPromptTemplate),分别给出用LangChain v1.0重新实现的方案。请说明是否能解决"把简单的事情搞复杂"的问题。如果能解决,请提供相应场景的可运行代码。
下面是那篇批评章节：【
### 5.2.1 为什么不推荐 LangChain

无论是 OpenAI 的 GPT 模型还是 Google 的 Gemini 模型，官方提供的能力都存在上限，我们常需要在社区中寻找突破点。

LangChain 作为当今最热门的框架之一，在 AI 应用开发中占据显著位置。但近来些年业内对 LangChain 的批评逐渐增多，LangChain 的劣势也日益凸显。

最直观的便是 Thoughtworks 技术雷达（Technology Radar）对 LangChain 态度的变化。技术雷达是咨询公司 Thoughtworks 每年两次定期发布的技术趋势报告，对新兴技术的成熟度与潜力进行评估，将技术分为试验、评估、采纳、淘汰等成熟度状态，为技术选型提供参考。LangChain 于 2023 年 4 月入选第 28 期技术雷达，成熟度状态为“评估”；在 2024 年 4 月第 30 期技术雷达中成熟度被降至 “暂缓”，即不推荐在项目中引入；在 2024 年 10 月第 31 期技术雷达中，LangChain 被彻底移除。LangChain 被淘汰的理由在它的成熟度被降至 “暂缓” 时，技术雷达给出了详细说明：“……我们还发现其存在 API 设计不一致且冗长的情况。因此，它经常会掩盖底层实际发生的情况，使开发者难以理解和控制 LLM 及其周围的各种模式的实际工作方式…”

这里所说的 API 并不是狭义上的 HTTP API，而是指广义上的代码间的通信接口。考虑到 API 是代码与框架的主要交互方式，对开发者而言，API 设计欠佳将使项目与框架集成之路变得困难重重。这里的批评并非空穴来风，接下来我用一个例子来说明 LangChain API 存在的问题。

在提示工程中有一类技巧叫作少样本学习（few shot learning），即在编写的提示语中有意包含一些示例，便于大模型更好地理解我们的意图以给出更精准的回答。例如，在向大模型询问 “big” 的反义词时，在提示语中提前给出反义词的相关示例，具体提示如下：

```
Give the antonym of every input
Input: happy
Output: sad
Input: tall
Output: short
Input: energetic
Output: lethargic
Input: sunny
Output: gloomy
Input: windy
Output: calm
```

大模型在读到诸多示例之后，即刻就会领悟我们想要简短直接的回答，并且回答必须契合“Input/Output”这类格式。用原生的 Python 将上述提示语抽象为代码并非难事，实现代码如下：

```python
def generate_few_shot_prompt(pairs):
    prompt = "Give the antonym of every input\n\n"
    for pair in pairs:
        input_word = pair['input']
        output_word = pair['output']
        prompt += f"Input: {input_word} \nOutput: {output_word}\n\n"
    return prompt
```

在调用该函数时，只需要将所有示例的 `input` 和 `output` 值组装成字典结构，然后以数组形式传入函数中即可：

```python
pairs = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
    {"input": "big", "output": ""}
]

few_shot_prompt = generate_few_shot_prompt(pairs)
```

现在看看使用 LangChain 框架如何实现上述功能。LangChain 为少样本学习这类业务场景提供了 `FewShotPromptTemplate` 提示模板，借助这个模板的代码实现如下：

```python
from langchain_core.example_selectors \
    import LengthBasedExampleSelector

from langchain_core.prompts \
    import FewShotPromptTemplate, PromptTemplate

examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"}
]

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input} \nOutput: {output}"
)

prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective} \nOutput:",
    input_variables=["adjective"]
)

print(prompt.format(adjective="big"))
```

我们对上述代码稍作分析，以便理解在 LangChain 中是如何实现少样本学习功能的。
- 需要导入 `PromptTemplate` 和 `FewShotPromptTemplate` 组件。
- `PromptTemplate` 组件用于创建常规提示，包含模板内容与变量定义，它与少样本学习场景无关。
- `FewShotPromptTemplate` 组件虽然从名称上同为“模板”，但实际上它更像是原模板的“封装”或者“改善”工具，因为从使用方式上看，它不仅为原模板补充了前缀部分（`prefix`）和提问部分（`suffix`），还负责将诸多示例（`examples`）注入提示语中。

也就是说，为了生成一个带有特定模式的提示语，在 LangChain 中我们需要导入两个模板组件（`PromptTemplate` 和 `FewShotPromptTemplate`），定义 3 个模板变量（`PromptTemplate` 中的 `input` 和 `output`，`FewShotPromptTemplate` 中的 `adjective`），还需要保证示例中数据格式与模板的变量格式相匹配。

很显然，这么做过于复杂了，别忘了我们只是想生成一个具有特定模式的字符串而已。在第 3 章的代码中可以看到，无须学习额外知识，更短的代码就可以达到同样的效果。

将少样本学习固化在代码中的做法有待商榷，因为少样本学习、思维链（chain of thought）、最少到最多提示（least to most prompting）等各类提示语技巧，本质上是解决问题的不同思路，它们为代码实现提供了多种可能性。而将思路固化在框架中与提供多种可能性的初衷背道而驰，给技巧的发挥套上了枷锁。

不仅如此，LangChain 为了支持将模板作为参数传入，还提供了与 `FewShotPromptTemplate` 几乎一样的组件 `FewShotPromptWithTemplates`，允许将原字符串类型的 `prefix` 和 `suffix` 变量替换为模板，这进一步增加了复杂度：

```python
# 在这里 prefix 与 suffix 不再是上段代码中简单的字符串，
# 而是被定义为可以用于生成提示语的模板
prefix_template = PromptTemplate(
    input_variables=["say_hi"],
    template="{say_hi}, Give the antonym of every input\n\n"
)
suffix_template = PromptTemplate(
    input_variables=["adjective"],
    template="Input: {adjective} \nOutput:"
)
few_shot_prompt_with_templates = FewShotPromptWithTemplates(
    #……
    input_variables=["adjective", "say_hi"],
    prefix=prefix_template,
    suffix=suffix_template
)
new_input = {
    "adjective": "big",
    "say_hi": "Hello"
}
prompt = few_shot_prompt_with_templates.format(**new_input)
```

在我看来这种设计存在明显问题。正确的做法应该是让组件支持更多可能性（如前端 DOM 选择器的 `querySelector()` 方法），而非为不同的可能性提供多个相似组件。这种设计给框架的维护者和使用者带来诸多不便。对维护者而言，这意味着他需要更频繁且更大幅度地对框架进行更新（这也是 LangChain API 文档更新不及时的原因之一，很多 API 在官网上只能找到参数说明，却找不到使用示例）；对使用者而言，这增加了框架的使用难度，很难精准找到适用的 API。这种设计变相降低了框架的容错性，也与我们之前所说的“容易让人把事情做对”的原则相违背。

有几篇对 LangChain 批评的文章影响力颇大，如“why we no longer use LangChain for building our AI agents”和“The Problem With LangChain”，前者在 Hacker News 上引起了广泛的讨论，后者被 Thoughtworks 技术雷达引用。如果大家有兴趣可以通过这些材料更进一步地了解 LangChain 当前存在的其他问题。

最后需要说明的是，对 LangChain 的定义存在广义和狭义之分。狭义上的 LangChain 指本章中讨论的 LangChain 框架，而广义上的 LangChain 代表 LangChain 技术品牌，旗下包含 LangChain、LangSmith（调试、测试、监控 AI 应用的平台）和 LangGraph（创建复杂工作流的 AI 应用的框架）3 类产品。本节讨论的各种问题仅针对 LangChain 框架本身。
】
```

## Claude app with sonnet 4.5:

```markdown
我来针对这篇2025年图书章节中的批评，用LangChain v1.0分析并提供解决方案。

## 核心批评点分析

这篇文章的批评集中在**FewShotPromptTemplate的过度设计**，让我逐一分析并提供v1.0的解决方案。

---

## 1. FewShotPromptTemplate - 基础场景

**原文批评**：生成一个带示例的提示语，原生Python只需简单函数，LangChain却需要两个模板组件、3个变量定义。

### LangChain v1.0 解决方案

#### 方案A：最简方式（推荐）

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 直接在模板中写示例，无需FewShotPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ("system", """Give the antonym of every input

Input: happy
Output: sad
Input: tall
Output: short
Input: energetic
Output: lethargic
Input: sunny
Output: gloomy
Input: windy
Output: calm"""),
    ("user", "Input: {adjective}\nOutput:")
])

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
chain = prompt | llm

result = chain.invoke({"adjective": "big"})
print(result.content)  # "small" 或 "little"
```

**是否解决问题**：✅ **完全解决**
- 只需一个`ChatPromptTemplate`
- 示例直接写在system消息中，清晰直观
- 不需要`FewShotPromptTemplate`、`PromptTemplate`等多层嵌套

---

#### 方案B：动态示例（如果确实需要）

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 如果示例需要动态生成
def format_examples(examples):
    formatted = []
    for ex in examples:
        formatted.append(f"Input: {ex['input']}\nOutput: {ex['output']}")
    return "\n".join(formatted)

examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"}
]

# 使用partial来预填充examples
prompt = ChatPromptTemplate.from_messages([
    ("system", "Give the antonym of every input\n\n{examples}"),
    ("user", "Input: {adjective}\nOutput:")
])

# 使用partial预填充示例
prompt_with_examples = prompt.partial(examples=format_examples(examples))

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
chain = prompt_with_examples | llm

result = chain.invoke({"adjective": "big"})
print(result.content)
```

**是否解决问题**：✅ **是的**
- 只需要一个简单的格式化函数
- 使用`.partial()`方法预填充，比旧版更优雅
- 不需要`FewShotPromptTemplate`

---

#### 方案C：如果仍想使用FewShotPromptTemplate（不推荐）

```python
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
]

# v1.0中仍然存在，但使用方式略有改进
example_prompt = PromptTemplate.from_template(
    "Input: {input}\nOutput: {output}"
)

prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"]
)

print(prompt.format(adjective="big"))
```

**是否解决问题**：❌ **没有解决**
- v1.0中`FewShotPromptTemplate`仍然存在且几乎没变
- 仍然需要嵌套的`PromptTemplate`
- 复杂度依旧

**结论**：不要用`FewShotPromptTemplate`，用方案A或B

---

## 2. FewShotPromptWithTemplates - 更复杂的场景

**原文批评**：为了支持prefix/suffix作为模板，又提供了`FewShotPromptWithTemplates`，增加复杂度。

### LangChain v1.0 解决方案

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 场景：prefix和suffix都需要变量
examples_text = """Input: happy
Output: sad
Input: tall
Output: short"""

# 使用单一ChatPromptTemplate，支持所有变量
prompt = ChatPromptTemplate.from_messages([
    ("system", "{greeting}, Give the antonym of every input\n\n{examples}"),
    ("user", "Input: {adjective}\nOutput:")
])

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
chain = prompt | llm

# 一次性传入所有变量
result = chain.invoke({
    "greeting": "Hello",
    "examples": examples_text,
    "adjective": "big"
})
print(result.content)
```

**是否解决问题**：✅ **完全解决**
- 不需要`FewShotPromptWithTemplates`这个组件
- `ChatPromptTemplate`天然支持任意数量的变量
- prefix、examples、suffix都只是模板中的普通变量

---

## 3. 完整对比：原生Python vs LangChain旧版 vs v1.0

### 场景：生成带动态greeting和动态示例的few-shot prompt

#### 原生Python实现

```python
def generate_few_shot_prompt(greeting, examples, question):
    prompt = f"{greeting}, Give the antonym of every input\n\n"
    for ex in examples:
        prompt += f"Input: {ex['input']}\nOutput: {ex['output']}\n"
    prompt += f"\nInput: {question}\nOutput:"
    return prompt

examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
]

prompt = generate_few_shot_prompt("Hello", examples, "big")
print(prompt)
```

**代码行数**：~10行
**复杂度**：⭐ 最简单

---

#### LangChain 旧版实现（书中批评的版本）

```python
from langchain_core.prompts import (
    FewShotPromptWithTemplates, 
    PromptTemplate
)

examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
]

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}"
)

prefix_template = PromptTemplate(
    input_variables=["greeting"],
    template="{greeting}, Give the antonym of every input\n"
)

suffix_template = PromptTemplate(
    input_variables=["adjective"],
    template="Input: {adjective}\nOutput:"
)

prompt = FewShotPromptWithTemplates(
    examples=examples,
    example_prompt=example_prompt,
    prefix=prefix_template,
    suffix=suffix_template,
    input_variables=["greeting", "adjective"]
)

result = prompt.format(greeting="Hello", adjective="big")
print(result)
```

**代码行数**：~30行
**复杂度**：⭐⭐⭐⭐⭐ 非常复杂
**问题**：
- 3个`PromptTemplate`嵌套
- 需要理解`FewShotPromptWithTemplates`的工作原理
- 变量分散在多个地方定义

---

#### LangChain v1.0 最佳实践

```python
from langchain_core.prompts import ChatPromptTemplate

def format_examples(examples):
    return "\n".join([f"Input: {ex['input']}\nOutput: {ex['output']}" 
                      for ex in examples])

examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
]

prompt = ChatPromptTemplate.from_messages([
    ("system", "{greeting}, Give the antonym of every input\n\n{examples}"),
    ("user", "Input: {adjective}\nOutput:")
])

result = prompt.format_messages(
    greeting="Hello",
    examples=format_examples(examples),
    adjective="big"
)

print(result[0].content + "\n" + result[1].content)
```

**代码行数**：~15行
**复杂度**：⭐⭐ 简单
**优势**：
- 只需一个`ChatPromptTemplate`
- 清晰的变量定义
- 使用标准的Python函数处理格式化

---

## 4. 实际可运行的完整示例

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 示例数据
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"}
]

# 方式1：静态示例（最简单）
print("=== 方式1：静态示例 ===")
prompt1 = ChatPromptTemplate.from_messages([
    ("system", """Give the antonym of every input

Input: happy
Output: sad
Input: tall  
Output: short
Input: energetic
Output: lethargic
Input: sunny
Output: gloomy
Input: windy
Output: calm"""),
    ("user", "Input: {adjective}\nOutput:")
])

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
chain1 = prompt1 | llm
result1 = chain1.invoke({"adjective": "big"})
print(result1.content)

# 方式2：动态示例
print("\n=== 方式2：动态示例 ===")
def format_examples(examples):
    return "\n".join([f"Input: {ex['input']}\nOutput: {ex['output']}" 
                      for ex in examples])

prompt2 = ChatPromptTemplate.from_messages([
    ("system", "Give the antonym of every input\n\n{examples}"),
    ("user", "Input: {adjective}\nOutput:")
])

chain2 = prompt2 | llm
result2 = chain2.invoke({
    "examples": format_examples(examples),
    "adjective": "big"
})
print(result2.content)

# 方式3：带greeting的动态示例
print("\n=== 方式3：带greeting的动态示例 ===")
prompt3 = ChatPromptTemplate.from_messages([
    ("system", "{greeting}! Give the antonym of every input\n\n{examples}"),
    ("user", "Input: {adjective}\nOutput:")
])

chain3 = prompt3 | llm
result3 = chain3.invoke({
    "greeting": "Hello",
    "examples": format_examples(examples),
    "adjective": "big"
})
print(result3.content)

# 方式4：使用partial预填充examples（推荐用于示例固定的场景）
print("\n=== 方式4：使用partial ===")
prompt4 = ChatPromptTemplate.from_messages([
    ("system", "Give the antonym of every input\n\n{examples}"),
    ("user", "Input: {adjective}\nOutput:")
])

# 预填充examples，之后只需传入adjective
prompt4_with_examples = prompt4.partial(examples=format_examples(examples))
chain4 = prompt4_with_examples | llm

result4 = chain4.invoke({"adjective": "big"})
print(result4.content)

# 测试多个不同的输入
for word in ["small", "hot", "fast"]:
    result = chain4.invoke({"adjective": word})
    print(f"{word} -> {result.content}")
```

---

## 5. 更高级场景：动态选择示例

如果确实需要根据输入动态选择示例（这是`FewShotPromptTemplate`原本想解决的场景），v1.0的推荐做法：

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda

# 所有可用示例
all_examples = [
    {"input": "happy", "output": "sad", "category": "emotion"},
    {"input": "tall", "output": "short", "category": "size"},
    {"input": "energetic", "output": "lethargic", "category": "energy"},
    {"input": "sunny", "output": "gloomy", "category": "weather"},
    {"input": "windy", "output": "calm", "category": "weather"},
    {"input": "hot", "output": "cold", "category": "temperature"},
]

# 根据类别选择示例
def select_examples_by_category(inputs):
    """根据输入的类别选择相关示例"""
    category = inputs.get("category", "emotion")
    selected = [ex for ex in all_examples if ex["category"] == category][:3]
    
    examples_text = "\n".join([
        f"Input: {ex['input']}\nOutput: {ex['output']}" 
        for ex in selected
    ])
    
    return {
        **inputs,
        "examples": examples_text
    }

# 构建chain
prompt = ChatPromptTemplate.from_messages([
    ("system", "Give the antonym of every input\n\n{examples}"),
    ("user", "Input: {adjective}\nOutput:")
])

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# 使用LCEL组合：先选择示例，再生成prompt，最后调用LLM
chain = (
    RunnableLambda(select_examples_by_category)
    | prompt
    | llm
)

# 测试不同类别
print("Weather category:")
result1 = chain.invoke({"adjective": "rainy", "category": "weather"})
print(result1.content)

print("\nSize category:")
result2 = chain.invoke({"adjective": "big", "category": "size"})
print(result2.content)
```

**是否解决问题**：✅ **是的**
- 使用LCEL的组合能力（`RunnableLambda`）
- 逻辑清晰：选择示例 → 格式化prompt → 调用LLM
- 比`LengthBasedExampleSelector`等内置选择器更灵活、更易理解

---

## 6. 完整对比总结表

| 维度 | 原生Python | LangChain旧版 | LangChain v1.0 | 是否解决问题 |
|------|-----------|--------------|---------------|------------|
| **静态示例** | f-string, 5行 | FewShotPromptTemplate, 20行 | ChatPromptTemplate, 8行 | ✅ 大幅简化 |
| **动态示例** | 函数+f-string, 10行 | FewShotPromptTemplate, 20行 | ChatPromptTemplate + 函数, 12行 | ✅ 简化 |
| **动态prefix/suffix** | 函数, 15行 | FewShotPromptWithTemplates, 35行 | ChatPromptTemplate, 15行 | ✅ 完全解决 |
| **动态选择示例** | 自定义逻辑, 20行 | ExampleSelector系列, 30行 | RunnableLambda + ChatPromptTemplate, 25行 | ⚠️ 改善但仍有复杂度 |
| **可读性** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ | ✅ 显著改善 |
| **需要学习的概念** | 0个 | 4-5个 | 1-2个 | ✅ 大幅减少 |

---

## 7. 核心结论

### 书中批评是否仍然有效？

**对于LangChain v1.0**: ⚠️ **部分有效**

✅ **已解决的问题**：
1. 不再需要`FewShotPromptTemplate`和`FewShotPromptWithTemplates`
2. `ChatPromptTemplate`统一处理所有场景
3. LCEL提供清晰的组合方式
4. 代码复杂度大幅降低

⚠️ **仍然存在的问题**：
1. `FewShotPromptTemplate`等组件仍在代码库中，初学者可能误用
2. 文档中仍然展示这些旧组件，造成混淆
3. 简单场景下，原生Python仍然更简单

### 最佳实践建议

**对于Few-Shot Learning场景**：

1. **简单静态示例** → 直接写在`ChatPromptTemplate`的system消息中
2. **动态示例** → 用普通Python函数格式化，配合`.partial()`或直接传入
3. **需要选择示例** → 用`RunnableLambda`包装选择逻辑
4. **永远不要用** `FewShotPromptTemplate`和`FewShotPromptWithTemplates`

### 对书中观点的回应

书中说："将少样本学习固化在代码中的做法有待商榷……给技巧的发挥套上了枷锁。"

**LangChain v1.0的改进**：
- ✅ `ChatPromptTemplate`不再固化特定技巧
- ✅ LCEL允许灵活组合
- ✅ 可以用普通Python逻辑实现任何提示技巧
- ⚠️ 但旧的固化组件仍然存在，造成生态混乱

**最终评价**：LangChain v1.0在设计哲学上有显著改进，但历史包袱仍在。**对于新项目，完全可以避开批评中提到的陷阱**。
```